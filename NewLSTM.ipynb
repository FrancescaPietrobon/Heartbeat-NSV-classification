{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonetome/ApplieAI-Biomedicine-Masciulli-Pietrobon-Tome/blob/main/OneVSAllProva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "markdown",
      "id": "fbff9da2",
      "metadata": {
        "id": "fbff9da2"
      },
      "source": [
        "The aim of this notebook is to show performances augmenting samples of the imbalanced class with random gaussian noise of zero mean and variance 0.05 as suggested in DOI: 10.1155/2020/3215681"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install heartpy\n",
        "!pip install PyWavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1CgC3jGsC2d",
        "outputId": "90bcd3d1-8ecf-45d9-e56a-ec0a972caf02"
      },
      "id": "l1CgC3jGsC2d",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: heartpy in /usr/local/lib/python3.7/dist-packages (1.2.7)\n",
            "Requirement already satisfied: matplotlib<=3.3.4 in /usr/local/lib/python3.7/dist-packages (from heartpy) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from heartpy) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from heartpy) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<=3.3.4->heartpy) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<=3.3.4->heartpy) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<=3.3.4->heartpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<=3.3.4->heartpy) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<=3.3.4->heartpy) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from PyWavelets) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7bfe12d7",
      "metadata": {
        "id": "7bfe12d7"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import zipfile \n",
        "from scipy.io import loadmat \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import io\n",
        "from PIL import Image\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "import sys\n",
        "from scipy.signal import resample, butter, lfilter, iirnotch\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import heartpy as hp\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "import pywt as pw\n",
        "import seaborn as sns\n",
        "SEED = 1\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IF Colab cloud GPU used -> Mount the My Drive folder and set cwd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cwd = '/content/drive/My Drive/FinalAssignment'\n",
        "os.chdir(cwd)\n",
        "dataset_folder = os.path.join(cwd,'training_set_dir')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiM7KQvEsJjV",
        "outputId": "a241c030-26dd-499b-baf5-7600646754d4"
      },
      "id": "iiM7KQvEsJjV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5fcdb248",
      "metadata": {
        "id": "5fcdb248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3a00ffa4-2e52-47bb-9668-38dbc695f6ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndataset_current_folder = \"training_set.zip\" # where the zip is\\ndataset_folder = \"C://Users//simon//Desktop//AppliedAI-project\" # where I want the dataset - avoid the current folder as Git doesn\\'t allow huge uploads\\n\\nwith zipfile.ZipFile(dataset_current_folder, \\'r\\') as zip: # extract the zip file into the desired folder \\n    zip.extractall(dataset_folder)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "dataset_current_folder = \"training_set.zip\" # where the zip is\n",
        "dataset_folder = \"C://Users//simon//Desktop//AppliedAI-project\" # where I want the dataset - avoid the current folder as Git doesn't allow huge uploads\n",
        "\n",
        "with zipfile.ZipFile(dataset_current_folder, 'r') as zip: # extract the zip file into the desired folder \n",
        "    zip.extractall(dataset_folder)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#IF local runtime used -> Set os.cwd as google drive volume , mounted locally\n",
        "os.chdir('G:\\Il mio Drive')\n",
        "cwd = os.getcwd()\n",
        "cwd = os.path.join(cwd,'Colab2022/AppliedAI/FinalAssignment')\n",
        "os.chdir(cwd)\n",
        "dataset_folder = os.path.join(cwd,'training_set_dir')\n",
        "'''"
      ],
      "metadata": {
        "id": "HsPnFitKvg0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cf88a8c1-51f8-41e3-8c37-e8e5e11d50fa"
      },
      "id": "HsPnFitKvg0j",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#IF local runtime used -> Set os.cwd as google drive volume , mounted locally\\nos.chdir('G:\\\\Il mio Drive')\\ncwd = os.getcwd()\\ncwd = os.path.join(cwd,'Colab2022/AppliedAI/FinalAssignment')\\nos.chdir(cwd)\\ndataset_folder = os.path.join(cwd,'training_set_dir')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "26e300ba",
      "metadata": {
        "id": "26e300ba"
      },
      "outputs": [],
      "source": [
        "def load_data(sample_prefix, input_dir):    # everything is returned as a numpy array which is easier to manipulate\n",
        "    label_filepath = os.path.join(input_dir, sample_prefix + '_ann.mat')\n",
        "    peak_filepath = os.path.join(input_dir, sample_prefix + '_rpk.mat')\n",
        "    signal_filepath = os.path.join(input_dir, sample_prefix + '.mat')\n",
        "    if os.path.isfile(label_filepath):\n",
        "        mat_file = loadmat(label_filepath)\n",
        "        label = np.asarray(mat_file['labels'])\n",
        "    if os.path.isfile(peak_filepath):\n",
        "        mat_file = loadmat(peak_filepath)\n",
        "        peaks = np.array(mat_file['rpeaks'],dtype=np.int64)\n",
        "    if os.path.isfile(signal_filepath):\n",
        "        mat_file = loadmat(signal_filepath)\n",
        "        signal = np.asarray(mat_file['ecg'] )\n",
        "\n",
        "    return label, peaks, signal\n",
        "\n",
        "labels = list()             # labels \n",
        "ids = list()                # Id of samples \n",
        "rpeaks = list()             # detected peaks of the signal \n",
        "ecg_signals = list()        # .mat ecg signal \n",
        "frequencies = list()        # sample frequency of the ecg signal \n",
        "\n",
        "\n",
        "for f in os.listdir(dataset_folder):\n",
        "  if f.lower().endswith('.mat'):\n",
        "    id = f[:4]\n",
        "    if id not in ids:\n",
        "      ids.append(id)\n",
        "      sample_prefix = f[:8]\n",
        "      label, peak, signal = load_data(sample_prefix, dataset_folder)\n",
        "      labels.append(label)\n",
        "      rpeaks.append(peak)\n",
        "      ecg_signals.append(signal)\n",
        "      frequencies.append(int(sample_prefix[5:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d704c9dd",
      "metadata": {
        "id": "d704c9dd",
        "outputId": "94d326f8-f4c1-4416-bd4d-8abce146b7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:00<00:00, 677.17it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8e22571a-f324-47e4-bb34-82ed349b00a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sigId</th>\n",
              "      <th>ecg_lead_1</th>\n",
              "      <th>ecg_lead_2</th>\n",
              "      <th>peaks</th>\n",
              "      <th>frequencies</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S001</td>\n",
              "      <td>[0.0, 0.04, 0.03, 0.0, 0.03, 0.09, 0.18, 0.14,...</td>\n",
              "      <td>[0.08, 0.07, 0.1, 0.06, 0.06, 0.03, 0.1, 0.21,...</td>\n",
              "      <td>[29, 110, 191, 272, 353, 433, 514, 595, 676, 7...</td>\n",
              "      <td>128</td>\n",
              "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S002</td>\n",
              "      <td>[-0.035, -0.045, -0.025, -0.035, -0.045, -0.05...</td>\n",
              "      <td>[-0.095, -0.105, -0.095, -0.095, -0.115, -0.09...</td>\n",
              "      <td>[48, 153, 243, 352, 440, 547, 636, 742, 831, 9...</td>\n",
              "      <td>128</td>\n",
              "      <td>[N, N, N, N, S, N, S, N, S, N, S, N, S, N, S, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S003</td>\n",
              "      <td>[-0.56, -0.56, -0.55, -0.47, -0.53, -0.47, -0....</td>\n",
              "      <td>[0.43, 0.56, 0.6, 0.41, 0.54, 0.48, 0.56, 0.46...</td>\n",
              "      <td>[91, 209, 326, 394, 537, 653, 745, 872, 984, 1...</td>\n",
              "      <td>128</td>\n",
              "      <td>[N, N, N, S, N, N, S, N, N, S, N, N, N, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S004</td>\n",
              "      <td>[-0.46, -0.49, -0.52, -0.58, -0.62, -0.69, -0....</td>\n",
              "      <td>[0.56, 0.61, 0.66, 0.66, 0.63, 0.66, 0.59, 0.5...</td>\n",
              "      <td>[98, 223, 349, 474, 599, 726, 853, 980, 1116, ...</td>\n",
              "      <td>128</td>\n",
              "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S005</td>\n",
              "      <td>[-0.27, -0.17, -0.13, -0.23, -0.18, -0.23, -0....</td>\n",
              "      <td>[-0.02, -0.04, -0.01, -0.01, -0.02, -0.06, 0.0...</td>\n",
              "      <td>[27, 127, 225, 324, 423, 523, 623, 722, 822, 9...</td>\n",
              "      <td>128</td>\n",
              "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e22571a-f324-47e4-bb34-82ed349b00a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e22571a-f324-47e4-bb34-82ed349b00a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e22571a-f324-47e4-bb34-82ed349b00a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  sigId  ...                                             labels\n",
              "0  S001  ...  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...\n",
              "1  S002  ...  [N, N, N, N, S, N, S, N, S, N, S, N, S, N, S, ...\n",
              "2  S003  ...  [N, N, N, S, N, N, S, N, N, S, N, N, N, N, N, ...\n",
              "3  S004  ...  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...\n",
              "4  S005  ...  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "cols = [\"sigId\",\"ecg_lead_1\",\"ecg_lead_2\",\"peaks\",\"frequencies\",\"labels\"]\n",
        "\n",
        "# ecg signals is 105 rows [,,,,,]\n",
        "\n",
        "first_lead_signals = []\n",
        "second_lead_signals = []\n",
        "\n",
        "for signal in ecg_signals:\n",
        "    first_lead_signals.append(signal[:,0].tolist())    # converting the array to list as list of array is deprecated \n",
        "    second_lead_signals.append(signal[:,1].tolist())\n",
        "\n",
        "df = pd.DataFrame(data =[ids,first_lead_signals,second_lead_signals,rpeaks,frequencies,labels]).T\n",
        "df.columns = cols\n",
        "\n",
        "# transform peaks \n",
        "for id in tqdm(df.index.tolist()):\n",
        "    peaks = df.iloc[id]['peaks']\n",
        "    p_list = list()\n",
        "    for p in peaks:\n",
        "        p_list.append(p[0])\n",
        "    df.iloc[id]['peaks'] = np.asarray(p_list).astype(np.int64)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "999717a8",
      "metadata": {
        "id": "999717a8",
        "outputId": "399af0d2-9837-4923-cddc-f4700d4f9dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nids_128 = df[df['frequencies'] == 128].index.tolist() \\nids_250 = df[df['frequencies'] == 250].index.tolist() \\nresampled_len = len(df.iloc[ids_128[0]]['ecg_lead_1'])\\nsampled_len = len(df.iloc[ids_250[0]]['ecg_lead_1'])\\n\\n\\nfor id in tqdm(ids_250):\\n    row = df.iloc[id]\\n    # first lead\\n    signal = np.asarray(row['ecg_lead_1']).astype(np.float32)\\n    resampled_1 = resample(signal,resampled_len)\\n    \\n    # second lead\\n    signal = np.asarray(row['ecg_lead_2']).astype(np.float32)\\n    resampled_2 = resample(signal,resampled_len)\\n    \\n    df.iloc[id]['ecg_lead_1'] = resampled_1.tolist()\\n    df.iloc[id]['ecg_lead_2'] = resampled_2.tolist()\\n    \\n    for i,p in enumerate(list(df.iloc[id]['peaks'])):\\n        df.iloc[id]['peaks'][i] = int(resampled_len * p/sampled_len)\\n\\ndf.head()\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# downsampling signal \n",
        "\n",
        "\n",
        "ids_128 = df[df['frequencies'] == 128].index.tolist() \n",
        "ids_250 = df[df['frequencies'] == 250].index.tolist() \n",
        "resampled_len = len(df.iloc[ids_128[0]]['ecg_lead_1'])\n",
        "sampled_len = len(df.iloc[ids_250[0]]['ecg_lead_1'])\n",
        "\n",
        "\n",
        "for id in tqdm(ids_250):\n",
        "    row = df.iloc[id]\n",
        "    # first lead\n",
        "    signal = np.asarray(row['ecg_lead_1']).astype(np.float32)\n",
        "    resampled_1 = resample(signal,resampled_len)\n",
        "    \n",
        "    # second lead\n",
        "    signal = np.asarray(row['ecg_lead_2']).astype(np.float32)\n",
        "    resampled_2 = resample(signal,resampled_len)\n",
        "    \n",
        "    df.iloc[id]['ecg_lead_1'] = resampled_1.tolist()\n",
        "    df.iloc[id]['ecg_lead_2'] = resampled_2.tolist()\n",
        "    \n",
        "    for i,p in enumerate(list(df.iloc[id]['peaks'])):\n",
        "        df.iloc[id]['peaks'][i] = int(resampled_len * p/sampled_len)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8a6e546",
      "metadata": {
        "id": "b8a6e546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "8a9c4d2f-057e-4061-be46-25213e6ad719"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef plot_signal(df,id,length):\\n    fig,axs = plt.subplots(2)\\n    fig.set_size_inches(18.5, 10.5)\\n    ax1 = axs[0]\\n    ax2 = axs[1]\\n    \\n    signal = df.iloc[id]\\n    \\n    first_lead = signal['ecg_lead_1'][:length]\\n    second_lead = signal['ecg_lead_2'][:length]\\n    \\n    peaks_ = signal['peaks']\\n    labels_ = signal['labels']\\n    \\n    x = list(range(length))\\n    \\n    ax1.plot(x,first_lead)\\n    ax2.plot(x,second_lead)\\n    \\n    peaks = list()\\n    p_amplitudes_first = list()\\n    p_amplitudes_second = list()\\n    labels = list()\\n    \\n    for i,p in enumerate(peaks_):\\n        if(p <= length):\\n            peaks.append(p)\\n            p_amplitudes_first.append(first_lead[p])\\n            p_amplitudes_second.append(second_lead[p])\\n            labels.append(labels_[i])\\n        else:\\n            break\\n            \\n    ax1.scatter(peaks,p_amplitudes_first,color='red')\\n    ax2.scatter(peaks,p_amplitudes_second,color='red')\\n    offset = 10\\n    \\n    for i, txt in enumerate(labels):\\n        ax1.annotate(txt, (peaks[i]+offset, p_amplitudes_first[i]),size = 14)\\n        ax2.annotate(txt, (peaks[i]+offset, p_amplitudes_second[i]),size = 14)\\n    \\n    print(labels)\\n    plt.show()\\n\\n\\n\\nplot_signal(df,0,1500)\\nplot_signal(df,1,1500)\\nplot_signal(df,2,1500)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# plot signal \n",
        "'''\n",
        "def plot_signal(df,id,length):\n",
        "    fig,axs = plt.subplots(2)\n",
        "    fig.set_size_inches(18.5, 10.5)\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "    \n",
        "    signal = df.iloc[id]\n",
        "    \n",
        "    first_lead = signal['ecg_lead_1'][:length]\n",
        "    second_lead = signal['ecg_lead_2'][:length]\n",
        "    \n",
        "    peaks_ = signal['peaks']\n",
        "    labels_ = signal['labels']\n",
        "    \n",
        "    x = list(range(length))\n",
        "    \n",
        "    ax1.plot(x,first_lead)\n",
        "    ax2.plot(x,second_lead)\n",
        "    \n",
        "    peaks = list()\n",
        "    p_amplitudes_first = list()\n",
        "    p_amplitudes_second = list()\n",
        "    labels = list()\n",
        "    \n",
        "    for i,p in enumerate(peaks_):\n",
        "        if(p <= length):\n",
        "            peaks.append(p)\n",
        "            p_amplitudes_first.append(first_lead[p])\n",
        "            p_amplitudes_second.append(second_lead[p])\n",
        "            labels.append(labels_[i])\n",
        "        else:\n",
        "            break\n",
        "            \n",
        "    ax1.scatter(peaks,p_amplitudes_first,color='red')\n",
        "    ax2.scatter(peaks,p_amplitudes_second,color='red')\n",
        "    offset = 10\n",
        "    \n",
        "    for i, txt in enumerate(labels):\n",
        "        ax1.annotate(txt, (peaks[i]+offset, p_amplitudes_first[i]),size = 14)\n",
        "        ax2.annotate(txt, (peaks[i]+offset, p_amplitudes_second[i]),size = 14)\n",
        "    \n",
        "    print(labels)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plot_signal(df,0,1500)\n",
        "plot_signal(df,1,1500)\n",
        "plot_signal(df,2,1500)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fc5a69f6",
      "metadata": {
        "id": "fc5a69f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2358d8d5-6cc5-4a0e-b214-4be6dd796cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:01<00:00, 25.41it/s]\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing used in practical lessons\n",
        "\n",
        "def notch_filter(cutoff, fs, q=30):\n",
        "    nyq = 0.5*fs\n",
        "    freq = cutoff/nyq\n",
        "    b, a = iirnotch(freq, q)\n",
        "    return b, a\n",
        "\n",
        "#def bandpass_filter(data, filter_order=5, lowcut = .5, highcut = 55, signal_freq=128):\n",
        "def bandpass_filter(data, filter_order=5, lowcut = .8, highcut = 45, signal_freq=128):# Bene V ma non S\n",
        "#def bandpass_filter(data, filter_order=5, lowcut = .8, highcut = 55, signal_freq=128):\n",
        "        \"\"\"\n",
        "        Method responsible for creating and applying Butterworth filter.\n",
        "        :param deque data: raw data\n",
        "        :param float lowcut: filter lowcut frequency value\n",
        "        :param float highcut: filter highcut frequency value\n",
        "        :param int signal_freq: signal frequency in samples per second (Hz)\n",
        "        :param int filter_order: filter order\n",
        "        :return array: filtered data\n",
        "        \"\"\"\n",
        "        powerline = 60\n",
        "        nyquist_freq = 0.5 * signal_freq\n",
        "        low = lowcut / nyquist_freq\n",
        "        high = highcut / nyquist_freq\n",
        "        b, a = butter(filter_order, [low, high], btype=\"band\")\n",
        "        y = lfilter(b, a, data)\n",
        "        #b,a = notch_filter(powerline,signal_freq)\n",
        "        #y = lfilter(b,a,data)\n",
        "        return y\n",
        "\n",
        "\n",
        "# center signals and remove baseline \n",
        "ids = df['sigId'].index\n",
        "for id in tqdm(ids):\n",
        "    row = df.iloc[id]\n",
        "    \n",
        "    first_sig = np.array(row['ecg_lead_1'])\n",
        "    second_sig = np.array(row['ecg_lead_2'])\n",
        "    \n",
        "    first_sig = bandpass_filter(first_sig)\n",
        "    second_sig = bandpass_filter(second_sig)\n",
        "\n",
        "    #first_sig = hp.enhance_ecg_peaks(first_sig, 128)\n",
        "    first_sig = hp.smooth_signal(first_sig, 128)\n",
        "    #first_sig = hp.remove_baseline_wander(first_sig, 128,cutoff=0.01)\n",
        "    #second_sig = hp.enhance_ecg_peaks(second_sig, 128)\n",
        "    second_sig = hp.smooth_signal(second_sig, 128)\n",
        "    #second_sig = hp.remove_baseline_wander(second_sig, 128,cutoff=0.01)\n",
        "    \n",
        "\n",
        "    # https://swharden.com/blog/2020-09-23-signal-filtering-in-python/\n",
        "    # create a normalized Hanning window\n",
        "    #windowSize = 5\n",
        "    #window = np.hanning(windowSize)\n",
        "    #window = window / window.sum()\n",
        "    #first_sig = np.convolve(window, first_sig, mode='same')\n",
        "    #second_sig = np.convolve(window, second_sig, mode='same')\n",
        "    #first_sig = ((first_sig - first_sig.min())/(first_sig.max()-first_sig.min())).tolist()\n",
        "    #second_sig = ((second_sig - second_sig.min())/(second_sig.max()-second_sig.min())).tolist()\n",
        "\n",
        "    #first_sig = ((first_sig - first_sig.mean())/(first_sig.std()))#.tolist()\n",
        "    #second_sig = ((second_sig - second_sig.mean())/(second_sig.std()))#.tolist()\n",
        "    df.iloc[id]['ecg_lead_1'] = first_sig[:]\n",
        "    df.iloc[id]['ecg_lead_2'] = second_sig[:]\n",
        "\n",
        "#plot_signal(df,0,1500)\n",
        "#plot_signal(df,1,1500)\n",
        "#plot_signal(df,2,1500)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "#del training_patches_df\n",
        "#del training_df\n",
        "#del validation_patches_df\n",
        "#del validation_df\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "Vy8VG8a_4V4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030a5ca4-d7d4-49c7-9a99-796fdfe693e1"
      },
      "id": "Vy8VG8a_4V4r",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "row = df.loc[48]\n",
        "len(row['peaks'])\n",
        "'''"
      ],
      "metadata": {
        "id": "TP8LdDZhiYx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eecd82b6-3caf-4ba4-b4bc-064ac19b2c94"
      },
      "id": "TP8LdDZhiYx7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nrow = df.loc[48]\\nlen(row['peaks'])\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# trying https://arxiv.org/pdf/1805.00794.pdf preprocessing and patch extraction\n",
        "\n",
        "def convert_to_one_hot(label):\n",
        "    return np.array(classes == label,dtype=np.float32)\n",
        "\n",
        "def create_patch_v2(df):\n",
        "  dataset_dict = {}\n",
        "  ids = df.index.tolist()\n",
        "  offset = 50\n",
        "  for id in tqdm(ids):\n",
        "      row =  df.loc[id]\n",
        "\n",
        "      peaks = row['peaks']\n",
        "      first_sig = row['ecg_lead_1']\n",
        "      second_sig = row['ecg_lead_2']\n",
        "      labels = row['labels']  \n",
        "      #1) Splitting the continuous ECG signal to 10s(1280 samples @128Hz) windows and select a 10s window from an ECG signal.\n",
        "      for i in range(0, min(len(first_sig), len(second_sig)), 1280):\n",
        "        first_sig_window = first_sig[i:i+1280]\n",
        "        second_sig_window = second_sig[i:i+1280]\n",
        "        #2) Normalizing the amplitude values to the range of between zero and one.\n",
        "        \n",
        "        #first_sig_std = (2*((first_sig_window - first_sig_window.min())/(first_sig_window.max()-first_sig_window.min()))-1).tolist()\n",
        "        #second_sig_std = (2*((second_sig_window - second_sig_window.min())/(second_sig_window.max()-second_sig_window.min()))-1).tolist()\n",
        "        ###\n",
        "        #first_sig_std = hp.remove_baseline_wander(first_sig_std, 128,cutoff=0.05)\n",
        "        #second_sig_std = hp.remove_baseline_wander(second_sig_std, 128,cutoff=0.05)\n",
        "        ###\n",
        "        #first_sig[i:i+1280] = first_sig_std\n",
        "        #second_sig[i:i+1280] = second_sig_std          \n",
        "        first_sig[i:i+1280] = first_sig_window\n",
        "        second_sig[i:i+1280] = second_sig_window\n",
        "\n",
        "      #3) Finding the set of all local maximums based on zerocrossings of the first derivative. NOT NEEDED\n",
        "      #4) Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. NOT NEEDED\n",
        "      #5) Finding the median of R-R time intervals as the nominal heartbeat period of that window (T).\n",
        "      T = 0\n",
        "      rr_list = list()\n",
        "\n",
        "\n",
        "      if(len(peaks) != len(labels)):\n",
        "          print(\"ERROR\")\n",
        "\n",
        "      for i,peak in enumerate(peaks): #compute each R-R time (in samples)\n",
        "        if i != len(peaks)-1:\n",
        "          rr = peaks[i+1]-peaks[i]\n",
        "          rr_list.append(rr)\n",
        "          \n",
        "      \n",
        "      import statistics\n",
        "\n",
        "      T = statistics.median(rr_list)\n",
        "      #6) For each R-peak, selecting a signal part with the length equal to 1.2T.\n",
        "      #patch_length=2*int(1.5*T)\n",
        "      #max_len = 500\n",
        "      \n",
        "      \n",
        "      ###\n",
        "\n",
        "      \n",
        "      max_len = 500\n",
        "      #6B) Setting last peak RR as T\n",
        "      rr_list.append(T)\n",
        "\n",
        "      l = len(peaks)\n",
        "      #7) Padding each selected part with zeros to make its length equal to a predefined fixed length.\n",
        "      for i,peak in enumerate(peaks):\n",
        "          if i == 0:\n",
        "            k = 0\n",
        "            h = peaks[i+1]+offset\n",
        "          elif i ==  l -1:\n",
        "            k = peaks[i-1]-offset\n",
        "            h = len(first_sig)\n",
        "        \n",
        "          else:\n",
        "            k = peaks[i-1]-offset\n",
        "            h = peaks[i+1]+offset\n",
        "\n",
        "          if k < 0:\n",
        "            k = 0\n",
        "          if h > len(first_sig):\n",
        "            h = len(first_sig)\n",
        "\n",
        "        \n",
        "          first_lead = first_sig[k:h] \n",
        "          second_lead = second_sig[k:h] \n",
        "\n",
        "\n",
        "          index = str(id)+'_'+str(i)\n",
        "          dataset_dict[index] = {}\n",
        "          \n",
        "          first_lead = (255*(first_lead - first_lead.min())/(first_lead.max()-first_lead.min())).tolist()\n",
        "          second_lead = (255*(second_lead - second_lead.min())/(second_lead.max()-second_lead.min())).tolist()\n",
        "          first_lead = resample(first_lead, max_len)\n",
        "          second_lead = resample(second_lead, max_len)\n",
        "          dataset_dict[index][\"first_lead\"] = first_lead\n",
        "          dataset_dict[index][\"second_lead\"] = second_lead\n",
        "          dataset_dict[index][\"RR_d\"] = rr_list[i]\n",
        "          dataset_dict[index][\"stringLabel\"] = labels[i]\n",
        "          dataset_dict[index][\"label\"] = convert_to_one_hot(labels[i])\n",
        "\n",
        "  dataset_df = pd.DataFrame.from_dict(dataset_dict,orient='index')\n",
        "  return dataset_df    \n",
        "\n",
        "classes = np.array([\"N\",\"S\",\"V\"])\n",
        "validation_percentage = 0.25\n",
        "training_df, validation_df = train_test_split(df,test_size = validation_percentage,random_state=SEED)\n",
        "training_patches_df = create_patch_v2(training_df)\n",
        "validation_patches_df = create_patch_v2(validation_df)\n",
        "validation_patches_df\n",
        "'''\n"
      ],
      "metadata": {
        "id": "qhZWe6DI3Zs0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "95beace1-050b-4bc0-8de4-a3c2c05cc8f2"
      },
      "id": "qhZWe6DI3Zs0",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# trying https://arxiv.org/pdf/1805.00794.pdf preprocessing and patch extraction\\n\\ndef convert_to_one_hot(label):\\n    return np.array(classes == label,dtype=np.float32)\\n\\ndef create_patch_v2(df):\\n  dataset_dict = {}\\n  ids = df.index.tolist()\\n  offset = 50\\n  for id in tqdm(ids):\\n      row =  df.loc[id]\\n\\n      peaks = row[\\'peaks\\']\\n      first_sig = row[\\'ecg_lead_1\\']\\n      second_sig = row[\\'ecg_lead_2\\']\\n      labels = row[\\'labels\\']  \\n      #1) Splitting the continuous ECG signal to 10s(1280 samples @128Hz) windows and select a 10s window from an ECG signal.\\n      for i in range(0, min(len(first_sig), len(second_sig)), 1280):\\n        first_sig_window = first_sig[i:i+1280]\\n        second_sig_window = second_sig[i:i+1280]\\n        #2) Normalizing the amplitude values to the range of between zero and one.\\n        \\n        #first_sig_std = (2*((first_sig_window - first_sig_window.min())/(first_sig_window.max()-first_sig_window.min()))-1).tolist()\\n        #second_sig_std = (2*((second_sig_window - second_sig_window.min())/(second_sig_window.max()-second_sig_window.min()))-1).tolist()\\n        ###\\n        #first_sig_std = hp.remove_baseline_wander(first_sig_std, 128,cutoff=0.05)\\n        #second_sig_std = hp.remove_baseline_wander(second_sig_std, 128,cutoff=0.05)\\n        ###\\n        #first_sig[i:i+1280] = first_sig_std\\n        #second_sig[i:i+1280] = second_sig_std          \\n        first_sig[i:i+1280] = first_sig_window\\n        second_sig[i:i+1280] = second_sig_window\\n\\n      #3) Finding the set of all local maximums based on zerocrossings of the first derivative. NOT NEEDED\\n      #4) Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. NOT NEEDED\\n      #5) Finding the median of R-R time intervals as the nominal heartbeat period of that window (T).\\n      T = 0\\n      rr_list = list()\\n\\n\\n      if(len(peaks) != len(labels)):\\n          print(\"ERROR\")\\n\\n      for i,peak in enumerate(peaks): #compute each R-R time (in samples)\\n        if i != len(peaks)-1:\\n          rr = peaks[i+1]-peaks[i]\\n          rr_list.append(rr)\\n          \\n      \\n      import statistics\\n\\n      T = statistics.median(rr_list)\\n      #6) For each R-peak, selecting a signal part with the length equal to 1.2T.\\n      #patch_length=2*int(1.5*T)\\n      #max_len = 500\\n      \\n      \\n      ###\\n\\n      \\n      max_len = 500\\n      #6B) Setting last peak RR as T\\n      rr_list.append(T)\\n\\n      l = len(peaks)\\n      #7) Padding each selected part with zeros to make its length equal to a predefined fixed length.\\n      for i,peak in enumerate(peaks):\\n          if i == 0:\\n            k = 0\\n            h = peaks[i+1]+offset\\n          elif i ==  l -1:\\n            k = peaks[i-1]-offset\\n            h = len(first_sig)\\n        \\n          else:\\n            k = peaks[i-1]-offset\\n            h = peaks[i+1]+offset\\n\\n          if k < 0:\\n            k = 0\\n          if h > len(first_sig):\\n            h = len(first_sig)\\n\\n        \\n          first_lead = first_sig[k:h] \\n          second_lead = second_sig[k:h] \\n\\n\\n          index = str(id)+\\'_\\'+str(i)\\n          dataset_dict[index] = {}\\n          \\n          first_lead = (255*(first_lead - first_lead.min())/(first_lead.max()-first_lead.min())).tolist()\\n          second_lead = (255*(second_lead - second_lead.min())/(second_lead.max()-second_lead.min())).tolist()\\n          first_lead = resample(first_lead, max_len)\\n          second_lead = resample(second_lead, max_len)\\n          dataset_dict[index][\"first_lead\"] = first_lead\\n          dataset_dict[index][\"second_lead\"] = second_lead\\n          dataset_dict[index][\"RR_d\"] = rr_list[i]\\n          dataset_dict[index][\"stringLabel\"] = labels[i]\\n          dataset_dict[index][\"label\"] = convert_to_one_hot(labels[i])\\n\\n  dataset_df = pd.DataFrame.from_dict(dataset_dict,orient=\\'index\\')\\n  return dataset_df    \\n\\nclasses = np.array([\"N\",\"S\",\"V\"])\\nvalidation_percentage = 0.25\\ntraining_df, validation_df = train_test_split(df,test_size = validation_percentage,random_state=SEED)\\ntraining_patches_df = create_patch_v2(training_df)\\nvalidation_patches_df = create_patch_v2(validation_df)\\nvalidation_patches_df\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# trying https://arxiv.org/pdf/1805.00794.pdf preprocessing and patch extraction\n",
        "\n",
        "def convert_to_one_hot(label):\n",
        "    return np.array(classes == label,dtype=np.float32)\n",
        "\n",
        "def create_patch_v2(df):\n",
        "  dataset_dict = {}\n",
        "  ids = df.index.tolist()\n",
        "  offset = 0\n",
        "  for id in tqdm(ids):\n",
        "      row =  df.loc[id]\n",
        "\n",
        "      peaks = row['peaks']\n",
        "      first_sig = row['ecg_lead_1']\n",
        "      second_sig = row['ecg_lead_2']\n",
        "      labels = row['labels']  \n",
        "      #1) Splitting the continuous ECG signal to 10s(1280 samples @128Hz) windows and select a 10s window from an ECG signal.\n",
        "      for i in range(0, min(len(first_sig), len(second_sig)), 1280):\n",
        "        first_sig_window = first_sig[i:i+1280]\n",
        "        second_sig_window = second_sig[i:i+1280]\n",
        "        #2) Normalizing the amplitude values to the range of between zero and one.\n",
        "        \n",
        "        #first_sig_std = (2*((first_sig_window - first_sig_window.min())/(first_sig_window.max()-first_sig_window.min()))-1).tolist()\n",
        "        #second_sig_std = (2*((second_sig_window - second_sig_window.min())/(second_sig_window.max()-second_sig_window.min()))-1).tolist()\n",
        "        ###\n",
        "        #first_sig_std = hp.remove_baseline_wander(first_sig_std, 128,cutoff=0.05)\n",
        "        #second_sig_std = hp.remove_baseline_wander(second_sig_std, 128,cutoff=0.05)\n",
        "        ###\n",
        "        #first_sig[i:i+1280] = first_sig_std\n",
        "        #second_sig[i:i+1280] = second_sig_std          \n",
        "        first_sig[i:i+1280] = first_sig_window\n",
        "        second_sig[i:i+1280] = second_sig_window\n",
        "\n",
        "      #3) Finding the set of all local maximums based on zerocrossings of the first derivative. NOT NEEDED\n",
        "      #4) Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. NOT NEEDED\n",
        "      #5) Finding the median of R-R time intervals as the nominal heartbeat period of that window (T).\n",
        "      T = 0\n",
        "      rr_list = list()\n",
        "\n",
        "\n",
        "      if(len(peaks) != len(labels)):\n",
        "          print(\"ERROR\")\n",
        "\n",
        "      for i,peak in enumerate(peaks): #compute each R-R time (in samples)\n",
        "        if i != len(peaks)-1:\n",
        "          rr = peaks[i+1]-peaks[i]\n",
        "          rr_list.append(rr)\n",
        "          \n",
        "      \n",
        "      import statistics\n",
        "\n",
        "      T = statistics.median(rr_list)\n",
        "      #6) For each R-peak, selecting a signal part with the length equal to 1.2T.\n",
        "      patch_length=2*int(1.5*T)\n",
        "      max_len = 500\n",
        "      \n",
        "      ###\n",
        "      #patch_length = 500\n",
        "      #max_len = 500\n",
        "      \n",
        "      #6B) Setting last peak RR as T\n",
        "      rr_list.append(T)\n",
        "      #7) Padding each selected part with zeros to make its length equal to a predefined fixed length.\n",
        "      for i,peak in enumerate(peaks):\n",
        "          index = str(id)+'_'+str(i)\n",
        "          dataset_dict[index] = {}\n",
        "          first_lead = list()\n",
        "          second_lead = list()\n",
        "\n",
        "          #size = list(range(peak-offset,int(max_len+peak)-offset))\n",
        "          size = list(range(peak+offset,int(peak+max_len)+offset))\n",
        "          if(len(size) != max_len):\n",
        "              print(\"error\")\n",
        "          for s in size:\n",
        "              if(s >= len(first_sig) or s >= (peak + patch_length/2)  or s < 0):   # padding with 0\n",
        "                  first_lead.append(0.) \n",
        "                  second_lead.append(0.) \n",
        "              else:\n",
        "                  first_lead.append(first_sig[s])\n",
        "                  second_lead.append(second_sig[s])\n",
        "          \n",
        "          first_lead = np.asarray(first_lead).astype(np.float32)\n",
        "          second_lead = np.asarray(second_lead).astype(np.float32)\n",
        "          first_lead = (255*(first_lead - first_lead.min())/(first_lead.max()-first_lead.min())).tolist()\n",
        "          second_lead = (255*(second_lead - second_lead.min())/(second_lead.max()-second_lead.min())).tolist()\n",
        "          dataset_dict[index][\"first_lead\"] = first_lead\n",
        "          dataset_dict[index][\"second_lead\"] = second_lead\n",
        "          dataset_dict[index][\"RR_d\"] = rr_list[i]\n",
        "          dataset_dict[index][\"stringLabel\"] = labels[i]\n",
        "          dataset_dict[index][\"label\"] = convert_to_one_hot(labels[i])\n",
        "\n",
        "  dataset_df = pd.DataFrame.from_dict(dataset_dict,orient='index')\n",
        "  return dataset_df    \n",
        "\n",
        "classes = np.array([\"N\",\"S\",\"V\"])\n",
        "validation_percentage = 0.2\n",
        "training_df, validation_df = train_test_split(df,test_size = validation_percentage,random_state=SEED)\n",
        "training_patches_df = create_patch_v2(training_df)\n",
        "validation_patches_df = create_patch_v2(validation_df)\n",
        "validation_patches_df\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "fYxFrxCUgEfz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "32ce70f7-daf2-4c00-e7e4-537942d01218"
      },
      "id": "fYxFrxCUgEfz",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# trying https://arxiv.org/pdf/1805.00794.pdf preprocessing and patch extraction\\n\\ndef convert_to_one_hot(label):\\n    return np.array(classes == label,dtype=np.float32)\\n\\ndef create_patch_v2(df):\\n  dataset_dict = {}\\n  ids = df.index.tolist()\\n  offset = 0\\n  for id in tqdm(ids):\\n      row =  df.loc[id]\\n\\n      peaks = row[\\'peaks\\']\\n      first_sig = row[\\'ecg_lead_1\\']\\n      second_sig = row[\\'ecg_lead_2\\']\\n      labels = row[\\'labels\\']  \\n      #1) Splitting the continuous ECG signal to 10s(1280 samples @128Hz) windows and select a 10s window from an ECG signal.\\n      for i in range(0, min(len(first_sig), len(second_sig)), 1280):\\n        first_sig_window = first_sig[i:i+1280]\\n        second_sig_window = second_sig[i:i+1280]\\n        #2) Normalizing the amplitude values to the range of between zero and one.\\n        \\n        #first_sig_std = (2*((first_sig_window - first_sig_window.min())/(first_sig_window.max()-first_sig_window.min()))-1).tolist()\\n        #second_sig_std = (2*((second_sig_window - second_sig_window.min())/(second_sig_window.max()-second_sig_window.min()))-1).tolist()\\n        ###\\n        #first_sig_std = hp.remove_baseline_wander(first_sig_std, 128,cutoff=0.05)\\n        #second_sig_std = hp.remove_baseline_wander(second_sig_std, 128,cutoff=0.05)\\n        ###\\n        #first_sig[i:i+1280] = first_sig_std\\n        #second_sig[i:i+1280] = second_sig_std          \\n        first_sig[i:i+1280] = first_sig_window\\n        second_sig[i:i+1280] = second_sig_window\\n\\n      #3) Finding the set of all local maximums based on zerocrossings of the first derivative. NOT NEEDED\\n      #4) Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. NOT NEEDED\\n      #5) Finding the median of R-R time intervals as the nominal heartbeat period of that window (T).\\n      T = 0\\n      rr_list = list()\\n\\n\\n      if(len(peaks) != len(labels)):\\n          print(\"ERROR\")\\n\\n      for i,peak in enumerate(peaks): #compute each R-R time (in samples)\\n        if i != len(peaks)-1:\\n          rr = peaks[i+1]-peaks[i]\\n          rr_list.append(rr)\\n          \\n      \\n      import statistics\\n\\n      T = statistics.median(rr_list)\\n      #6) For each R-peak, selecting a signal part with the length equal to 1.2T.\\n      patch_length=2*int(1.5*T)\\n      max_len = 500\\n      \\n      ###\\n      #patch_length = 500\\n      #max_len = 500\\n      \\n      #6B) Setting last peak RR as T\\n      rr_list.append(T)\\n      #7) Padding each selected part with zeros to make its length equal to a predefined fixed length.\\n      for i,peak in enumerate(peaks):\\n          index = str(id)+\\'_\\'+str(i)\\n          dataset_dict[index] = {}\\n          first_lead = list()\\n          second_lead = list()\\n\\n          #size = list(range(peak-offset,int(max_len+peak)-offset))\\n          size = list(range(peak+offset,int(peak+max_len)+offset))\\n          if(len(size) != max_len):\\n              print(\"error\")\\n          for s in size:\\n              if(s >= len(first_sig) or s >= (peak + patch_length/2)  or s < 0):   # padding with 0\\n                  first_lead.append(0.) \\n                  second_lead.append(0.) \\n              else:\\n                  first_lead.append(first_sig[s])\\n                  second_lead.append(second_sig[s])\\n          \\n          first_lead = np.asarray(first_lead).astype(np.float32)\\n          second_lead = np.asarray(second_lead).astype(np.float32)\\n          first_lead = (255*(first_lead - first_lead.min())/(first_lead.max()-first_lead.min())).tolist()\\n          second_lead = (255*(second_lead - second_lead.min())/(second_lead.max()-second_lead.min())).tolist()\\n          dataset_dict[index][\"first_lead\"] = first_lead\\n          dataset_dict[index][\"second_lead\"] = second_lead\\n          dataset_dict[index][\"RR_d\"] = rr_list[i]\\n          dataset_dict[index][\"stringLabel\"] = labels[i]\\n          dataset_dict[index][\"label\"] = convert_to_one_hot(labels[i])\\n\\n  dataset_df = pd.DataFrame.from_dict(dataset_dict,orient=\\'index\\')\\n  return dataset_df    \\n\\nclasses = np.array([\"N\",\"S\",\"V\"])\\nvalidation_percentage = 0.2\\ntraining_df, validation_df = train_test_split(df,test_size = validation_percentage,random_state=SEED)\\ntraining_patches_df = create_patch_v2(training_df)\\nvalidation_patches_df = create_patch_v2(validation_df)\\nvalidation_patches_df\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fixed patch\n",
        "\n",
        "def convert_to_one_hot(label):\n",
        "    return np.array(classes == label,dtype=np.float32)\n",
        "\n",
        "def create_patch_v2(df):\n",
        "  dataset_dict = {}\n",
        "  ids = df.index.tolist()\n",
        "  offset = 0\n",
        "  for id in tqdm(ids):\n",
        "      row =  df.loc[id]\n",
        "\n",
        "      peaks = row['peaks']\n",
        "      first_sig = row['ecg_lead_1']\n",
        "      second_sig = row['ecg_lead_2']\n",
        "      labels = row['labels']  \n",
        "      #1) Splitting the continuous ECG signal to 10s(1280 samples @128Hz) windows and select a 10s window from an ECG signal.\n",
        "      for i in range(0, min(len(first_sig), len(second_sig)), 1280):\n",
        "        first_sig_window = first_sig[i:i+1280]\n",
        "        second_sig_window = second_sig[i:i+1280]\n",
        "        #2) Normalizing the amplitude values to the range of between zero and one.\n",
        "        \n",
        "        #first_sig_std = (2*((first_sig_window - first_sig_window.min())/(first_sig_window.max()-first_sig_window.min()))-1).tolist()\n",
        "        #second_sig_std = (2*((second_sig_window - second_sig_window.min())/(second_sig_window.max()-second_sig_window.min()))-1).tolist()\n",
        "        ###\n",
        "        #first_sig_std = hp.remove_baseline_wander(first_sig_std, 128,cutoff=0.05)\n",
        "        #second_sig_std = hp.remove_baseline_wander(second_sig_std, 128,cutoff=0.05)\n",
        "        ###\n",
        "        #first_sig[i:i+1280] = first_sig_std\n",
        "        #second_sig[i:i+1280] = second_sig_std    \n",
        "        '''      \n",
        "        first_sig[i:i+1280] = first_sig_window\n",
        "        second_sig[i:i+1280] = second_sig_window\n",
        "        '''\n",
        "\n",
        "      #3) Finding the set of all local maximums based on zerocrossings of the first derivative. NOT NEEDED\n",
        "      #4) Finding the set of ECG R-peak candidates by applying a threshold of 0.9 on the normalized value of the local maximums. NOT NEEDED\n",
        "      #5) Finding the median of R-R time intervals as the nominal heartbeat period of that window (T).\n",
        "      T = 0\n",
        "      rr_list = list()\n",
        "\n",
        "      '''\n",
        "      if(len(peaks) != len(labels)):\n",
        "          print(\"ERROR\")\n",
        "      '''\n",
        "\n",
        "      for i,peak in enumerate(peaks): #compute each R-R time (in samples)\n",
        "        if i != len(peaks)-1:\n",
        "          rr = peaks[i+1]-peaks[i]\n",
        "          rr_list.append(rr)\n",
        "          \n",
        "      \n",
        "      import statistics\n",
        "\n",
        "      T = statistics.median(rr_list)\n",
        "      #6) For each R-peak, selecting a signal part with the length equal to 1.2T.\n",
        "      patch_length=2*int(1.5*T)\n",
        "      max_len = 500\n",
        "      \n",
        "      ###\n",
        "      #patch_length = 500\n",
        "      #max_len = 500\n",
        "      \n",
        "      #6B) Setting last peak RR as T\n",
        "      rr_list.append(T)\n",
        "      #7) Padding each selected part with zeros to make its length equal to a predefined fixed length.\n",
        "      for i,peak in enumerate(peaks):\n",
        "          index = str(id)+'_'+str(i)\n",
        "          dataset_dict[index] = {}\n",
        "          first_lead = list()\n",
        "          second_lead = list()\n",
        "\n",
        "          #size = list(range(peak-offset,int(max_len+peak)-offset))\n",
        "          size = list(range(int(peak-max_len/2)+offset,int(peak+max_len/2)+offset))\n",
        "          if(len(size) != max_len):\n",
        "              print(\"error\")\n",
        "          for s in size:\n",
        "              if(s >= len(first_sig) or s >= (peak + patch_length/2) or s <= (peak -patch_length/2) or s < 0):   # padding with 0\n",
        "                  first_lead.append(0.) \n",
        "                  second_lead.append(0.) \n",
        "              else:\n",
        "                  first_lead.append(first_sig[s])\n",
        "                  second_lead.append(second_sig[s])\n",
        "          \n",
        "          first_lead = np.asarray(first_lead).astype(np.float32)\n",
        "          second_lead = np.asarray(second_lead).astype(np.float32)\n",
        "          first_lead = (255*(first_lead - first_lead.min())/(first_lead.max()-first_lead.min())).tolist()\n",
        "          second_lead = (255*(second_lead - second_lead.min())/(second_lead.max()-second_lead.min())).tolist()\n",
        "          dataset_dict[index][\"first_lead\"] = first_lead\n",
        "          dataset_dict[index][\"second_lead\"] = second_lead\n",
        "          dataset_dict[index][\"RR_d\"] = rr_list[i]\n",
        "          dataset_dict[index][\"stringLabel\"] = labels[i]\n",
        "          dataset_dict[index][\"label\"] = convert_to_one_hot(labels[i])\n",
        "\n",
        "  dataset_df = pd.DataFrame.from_dict(dataset_dict,orient='index')\n",
        "  return dataset_df    \n",
        "\n",
        "classes = np.array([\"N\",\"S\",\"V\"])\n",
        "validation_percentage = 0.2\n",
        "training_df, validation_df = train_test_split(df,test_size = validation_percentage,random_state=SEED)\n",
        "training_patches_df = create_patch_v2(training_df)\n",
        "validation_patches_df = create_patch_v2(validation_df)\n",
        "validation_patches_df\n",
        "\n"
      ],
      "metadata": {
        "id": "cDIFJ1X0wUnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "3d243011-31f4-4b49-dfa0-ff87637ef7b4"
      },
      "id": "cDIFJ1X0wUnM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [02:57<00:00,  7.41s/it]\n",
            "100%|██████████| 6/6 [00:40<00:00,  6.79s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70dda432-334e-4780-a89f-e35d5f86de9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first_lead</th>\n",
              "      <th>second_lead</th>\n",
              "      <th>RR_d</th>\n",
              "      <th>stringLabel</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17_0</th>\n",
              "      <td>[118.2327651977539, 118.2327651977539, 118.232...</td>\n",
              "      <td>[130.4786376953125, 130.4786376953125, 130.478...</td>\n",
              "      <td>109.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_1</th>\n",
              "      <td>[118.2327651977539, 118.2327651977539, 118.232...</td>\n",
              "      <td>[126.50159454345703, 126.50159454345703, 126.5...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_2</th>\n",
              "      <td>[109.64065551757812, 109.64065551757812, 109.6...</td>\n",
              "      <td>[119.01765441894531, 119.01765441894531, 119.0...</td>\n",
              "      <td>74.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_3</th>\n",
              "      <td>[109.64065551757812, 109.64065551757812, 109.6...</td>\n",
              "      <td>[124.4853744506836, 124.4853744506836, 124.485...</td>\n",
              "      <td>110.0</td>\n",
              "      <td>S</td>\n",
              "      <td>[0.0, 1.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_4</th>\n",
              "      <td>[109.64065551757812, 109.64065551757812, 109.6...</td>\n",
              "      <td>[135.035888671875, 135.035888671875, 135.03588...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_1930</th>\n",
              "      <td>[122.96874237060547, 122.96874237060547, 122.9...</td>\n",
              "      <td>[130.68431091308594, 130.68431091308594, 130.6...</td>\n",
              "      <td>122.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_1931</th>\n",
              "      <td>[119.81330108642578, 119.81330108642578, 119.8...</td>\n",
              "      <td>[130.68431091308594, 130.68431091308594, 130.6...</td>\n",
              "      <td>125.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_1932</th>\n",
              "      <td>[121.8005142211914, 121.8005142211914, 121.800...</td>\n",
              "      <td>[129.02491760253906, 129.02491760253906, 129.0...</td>\n",
              "      <td>119.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_1933</th>\n",
              "      <td>[121.8005142211914, 121.8005142211914, 121.800...</td>\n",
              "      <td>[129.02491760253906, 129.02491760253906, 129.0...</td>\n",
              "      <td>123.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_1934</th>\n",
              "      <td>[121.8005142211914, 121.8005142211914, 121.800...</td>\n",
              "      <td>[126.25089263916016, 126.25089263916016, 126.2...</td>\n",
              "      <td>119.0</td>\n",
              "      <td>N</td>\n",
              "      <td>[1.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12913 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70dda432-334e-4780-a89f-e35d5f86de9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70dda432-334e-4780-a89f-e35d5f86de9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70dda432-334e-4780-a89f-e35d5f86de9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                first_lead  ...            label\n",
              "17_0     [118.2327651977539, 118.2327651977539, 118.232...  ...  [1.0, 0.0, 0.0]\n",
              "17_1     [118.2327651977539, 118.2327651977539, 118.232...  ...  [1.0, 0.0, 0.0]\n",
              "17_2     [109.64065551757812, 109.64065551757812, 109.6...  ...  [1.0, 0.0, 0.0]\n",
              "17_3     [109.64065551757812, 109.64065551757812, 109.6...  ...  [0.0, 1.0, 0.0]\n",
              "17_4     [109.64065551757812, 109.64065551757812, 109.6...  ...  [1.0, 0.0, 0.0]\n",
              "...                                                    ...  ...              ...\n",
              "20_1930  [122.96874237060547, 122.96874237060547, 122.9...  ...  [1.0, 0.0, 0.0]\n",
              "20_1931  [119.81330108642578, 119.81330108642578, 119.8...  ...  [1.0, 0.0, 0.0]\n",
              "20_1932  [121.8005142211914, 121.8005142211914, 121.800...  ...  [1.0, 0.0, 0.0]\n",
              "20_1933  [121.8005142211914, 121.8005142211914, 121.800...  ...  [1.0, 0.0, 0.0]\n",
              "20_1934  [121.8005142211914, 121.8005142211914, 121.800...  ...  [1.0, 0.0, 0.0]\n",
              "\n",
              "[12913 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "validation_patches_df[(validation_patches_df['stringLabel']=='S')]\n",
        "'''"
      ],
      "metadata": {
        "id": "tkQWUJ9YmSrT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcddd01c-4716-454c-eef4-875592e25659"
      },
      "id": "tkQWUJ9YmSrT",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nvalidation_patches_df[(validation_patches_df['stringLabel']=='S')]\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "    signal = validation_patches_df.loc[\"35_7\"]\n",
        "    length = len(signal['first_lead'])\n",
        "    fig,axs = plt.subplots(2)\n",
        "    fig.set_size_inches(18.5, 10.5)\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "  \n",
        "    \n",
        "    first_lead = signal['first_lead'][:length]\n",
        "    second_lead = signal['second_lead'][:length]\n",
        "    \n",
        "    x = list(range(length))\n",
        "    \n",
        "    ax1.plot(x,first_lead)\n",
        "    ax2.plot(x,second_lead)\n",
        "    \n",
        "    plt.show()\n",
        "    '''"
      ],
      "metadata": {
        "id": "whT4kvYPn_d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8502c808-0719-4232-be25-5469dc79dc55"
      },
      "id": "whT4kvYPn_d8",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsignal = validation_patches_df.loc[\"35_7\"]\\nlength = len(signal[\\'first_lead\\'])\\nfig,axs = plt.subplots(2)\\nfig.set_size_inches(18.5, 10.5)\\nax1 = axs[0]\\nax2 = axs[1]\\n  \\n\\nfirst_lead = signal[\\'first_lead\\'][:length]\\nsecond_lead = signal[\\'second_lead\\'][:length]\\n\\nx = list(range(length))\\n\\nax1.plot(x,first_lead)\\nax2.plot(x,second_lead)\\n\\nplt.show()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "    signal = validation_patches_df.loc[\"47_1704\"]\n",
        "    length = len(signal['first_lead'])\n",
        "    fig,axs = plt.subplots(2)\n",
        "    fig.set_size_inches(18.5, 10.5)\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "  \n",
        "    \n",
        "    first_lead = signal['first_lead'][:length]\n",
        "    second_lead = signal['second_lead'][:length]\n",
        "    \n",
        "    x = list(range(length))\n",
        "    \n",
        "    ax1.plot(x,first_lead)\n",
        "    ax2.plot(x,second_lead)\n",
        "    \n",
        "    plt.show()\n",
        "    '''"
      ],
      "metadata": {
        "id": "tlZvxtbv8ePE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1d513c94-3305-4dbb-f253-ac239c9c1421"
      },
      "id": "tlZvxtbv8ePE",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsignal = validation_patches_df.loc[\"47_1704\"]\\nlength = len(signal[\\'first_lead\\'])\\nfig,axs = plt.subplots(2)\\nfig.set_size_inches(18.5, 10.5)\\nax1 = axs[0]\\nax2 = axs[1]\\n  \\n\\nfirst_lead = signal[\\'first_lead\\'][:length]\\nsecond_lead = signal[\\'second_lead\\'][:length]\\n\\nx = list(range(length))\\n\\nax1.plot(x,first_lead)\\nax2.plot(x,second_lead)\\n\\nplt.show()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "validation_patches_df[(validation_patches_df['stringLabel']=='V')]\n",
        "'''"
      ],
      "metadata": {
        "id": "P2OtuXiuD6ie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f941b199-ea82-4e12-93b3-30caa0d0f132"
      },
      "id": "P2OtuXiuD6ie",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nvalidation_patches_df[(validation_patches_df['stringLabel']=='V')]\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "    signal = validation_patches_df.loc[\"57_1969\"]\n",
        "    length = len(signal['first_lead'])\n",
        "    fig,axs = plt.subplots(2)\n",
        "    fig.set_size_inches(18.5, 10.5)\n",
        "    ax1 = axs[0]\n",
        "    ax2 = axs[1]\n",
        "  \n",
        "    \n",
        "    first_lead = signal['first_lead'][:length]\n",
        "    second_lead = signal['second_lead'][:length]\n",
        "    \n",
        "    x = list(range(length))\n",
        "    \n",
        "    ax1.plot(x,first_lead)\n",
        "    ax2.plot(x,second_lead)\n",
        "    \n",
        "    plt.show()\n",
        "    '''"
      ],
      "metadata": {
        "id": "HyQNLqpGEC5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f7b58244-3787-4780-994d-0df5e034e423"
      },
      "id": "HyQNLqpGEC5V",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsignal = validation_patches_df.loc[\"57_1969\"]\\nlength = len(signal[\\'first_lead\\'])\\nfig,axs = plt.subplots(2)\\nfig.set_size_inches(18.5, 10.5)\\nax1 = axs[0]\\nax2 = axs[1]\\n  \\n\\nfirst_lead = signal[\\'first_lead\\'][:length]\\nsecond_lead = signal[\\'second_lead\\'][:length]\\n\\nx = list(range(length))\\n\\nax1.plot(x,first_lead)\\nax2.plot(x,second_lead)\\n\\nplt.show()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "  \"\"\"\n",
        "    :param ndarray y_true: contains the ground truth labels in the shape (n_samples,)\n",
        "    :param ndarray y_pred: contains the predicted labels in the shape (n_samples,)\n",
        "  \"\"\"\n",
        "  con_mat = tf.math.confusion_matrix(labels=np.squeeze(y_true), predictions=np.rint(np.squeeze(y_pred))).numpy()\n",
        "  con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "  con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                  index = classes, \n",
        "                  columns = classes)\n",
        "  figure = plt.figure(figsize=(8, 8))\n",
        "  sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8sMFEIwKEZjc"
      },
      "id": "8sMFEIwKEZjc",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(df):\n",
        "    \n",
        "    x = list()\n",
        "    y = list()\n",
        "    \n",
        "    for id in tqdm(df.index.tolist()):\n",
        "        row = df.loc[id]\n",
        "        x.append(np.transpose(np.asarray([row['first_lead'],row['second_lead']]).astype(np.float32)))\n",
        "        y.append(row['label'])\n",
        "\n",
        "    x = np.asarray(x).astype(np.float32)\n",
        "    y = np.asarray(y).astype(np.float32)\n",
        "    \n",
        "    return x,y"
      ],
      "metadata": {
        "id": "ie_8TDGvH6Ct"
      },
      "id": "ie_8TDGvH6Ct",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,y_train = create_input(training_patches_df)\n",
        "x_valid,y_valid = create_input(validation_patches_df)"
      ],
      "metadata": {
        "id": "vVQS64b1HOue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daea833f-d5d0-4ceb-e64b-4f6ffac50c0c"
      },
      "id": "vVQS64b1HOue",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59397/59397 [00:22<00:00, 2624.97it/s]\n",
            "100%|██████████| 12913/12913 [00:04<00:00, 2761.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "timesteps = 500 #patch size\n",
        "channels_num = 2\n",
        "\n",
        "input = keras.layers.Input(shape=(timesteps, channels_num))\n",
        "\n",
        "conv1 = keras.layers.Conv1D(filters=32,kernel_size=25,activation=\"relu\")(input)\n",
        "maxPool1 = keras.layers.MaxPool1D(pool_size=2)(conv1)\n",
        "dropout1 = keras.layers.Dropout(0.05)(maxPool1)\n",
        "conv2 = keras.layers.Conv1D(filters=64,kernel_size=12,activation=\"relu\")(dropout1)\n",
        "maxPool2 = keras.layers.MaxPool1D(pool_size=2)(conv2)\n",
        "dropout2 = keras.layers.Dropout(0.1)(maxPool2)\n",
        "conv3 = keras.layers.Conv1D(filters=128,kernel_size=9,activation=\"relu\")(dropout2)\n",
        "maxPool3 = keras.layers.MaxPool1D(pool_size=2)(conv3)\n",
        "dropout3 = keras.layers.Dropout(0.15)(maxPool3)\n",
        "lstm1 = keras.layers.LSTM(64, return_sequences=True)(dropout3)\n",
        "lstm2 = keras.layers.LSTM(64, return_sequences=False)(lstm1)\n",
        "output = keras.layers.Dense(3, activation='softmax',  name='softmax_classifier')(lstm2)\n",
        "model = keras.models.Model(inputs=input, outputs=output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,mode=\"auto\", restore_best_weights=True)\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 1e-5\n",
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy'],\n",
        "    \n",
        ")\n",
        "\n",
        "y_integers = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight(\n",
        "                                            class_weight = 'balanced', \n",
        "                                            classes =  np.unique(y_integers), \n",
        "                                            y = y_integers\n",
        "                                        )\n",
        "d_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "history = model.fit(\n",
        "    x = x_train,\n",
        "    y = y_train, \n",
        "    batch_size = batch_size,\n",
        "    steps_per_epoch=len(x_train)/batch_size,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    epochs = 100,\n",
        "    shuffle = True,\n",
        "    callbacks=[callback],\n",
        "    class_weight = d_class_weights\n",
        ").history\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h8J2KSIERUr",
        "outputId": "a4662bfd-1670-4691-8544-37d3da8309bd"
      },
      "id": "3h8J2KSIERUr",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 500, 2)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 476, 32)           1632      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 238, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 238, 32)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 227, 64)           24640     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 113, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 113, 64)           0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 105, 128)          73856     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 52, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 52, 128)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 52, 64)            49408     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " softmax_classifier (Dense)  (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 182,755\n",
            "Trainable params: 182,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "464/464 [==============================] - 27s 44ms/step - loss: 0.6482 - accuracy: 0.0972 - val_loss: 0.6495 - val_accuracy: 0.0845\n",
            "Epoch 2/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.6370 - accuracy: 0.2939 - val_loss: 0.6567 - val_accuracy: 0.0723\n",
            "Epoch 3/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.6356 - accuracy: 0.3837 - val_loss: 0.6691 - val_accuracy: 0.0651\n",
            "Epoch 4/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.6306 - accuracy: 0.4270 - val_loss: 0.6608 - val_accuracy: 0.0726\n",
            "Epoch 5/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.6131 - accuracy: 0.4951 - val_loss: 0.6647 - val_accuracy: 0.1644\n",
            "Epoch 6/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.5597 - accuracy: 0.5490 - val_loss: 0.6227 - val_accuracy: 0.5793\n",
            "Epoch 7/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.5193 - accuracy: 0.5937 - val_loss: 0.5741 - val_accuracy: 0.3807\n",
            "Epoch 8/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.4918 - accuracy: 0.6017 - val_loss: 0.5962 - val_accuracy: 0.3105\n",
            "Epoch 9/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.4737 - accuracy: 0.5832 - val_loss: 0.6039 - val_accuracy: 0.3183\n",
            "Epoch 10/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.4555 - accuracy: 0.6051 - val_loss: 0.5739 - val_accuracy: 0.4714\n",
            "Epoch 11/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.4390 - accuracy: 0.6213 - val_loss: 0.6244 - val_accuracy: 0.4336\n",
            "Epoch 12/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.4228 - accuracy: 0.6326 - val_loss: 0.5364 - val_accuracy: 0.6124\n",
            "Epoch 13/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.4042 - accuracy: 0.6522 - val_loss: 0.5277 - val_accuracy: 0.6248\n",
            "Epoch 14/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3932 - accuracy: 0.6703 - val_loss: 0.4769 - val_accuracy: 0.6738\n",
            "Epoch 15/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3818 - accuracy: 0.6769 - val_loss: 0.4982 - val_accuracy: 0.6499\n",
            "Epoch 16/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3688 - accuracy: 0.6882 - val_loss: 0.4802 - val_accuracy: 0.6802\n",
            "Epoch 17/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3569 - accuracy: 0.6983 - val_loss: 0.4360 - val_accuracy: 0.7293\n",
            "Epoch 18/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3458 - accuracy: 0.7093 - val_loss: 0.4174 - val_accuracy: 0.7395\n",
            "Epoch 19/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.3363 - accuracy: 0.7216 - val_loss: 0.4383 - val_accuracy: 0.7297\n",
            "Epoch 20/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3259 - accuracy: 0.7330 - val_loss: 0.4096 - val_accuracy: 0.7410\n",
            "Epoch 21/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3157 - accuracy: 0.7380 - val_loss: 0.4119 - val_accuracy: 0.7335\n",
            "Epoch 22/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3087 - accuracy: 0.7479 - val_loss: 0.4482 - val_accuracy: 0.7094\n",
            "Epoch 23/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.3014 - accuracy: 0.7548 - val_loss: 0.4140 - val_accuracy: 0.7405\n",
            "Epoch 24/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2909 - accuracy: 0.7680 - val_loss: 0.4210 - val_accuracy: 0.7273\n",
            "Epoch 25/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2837 - accuracy: 0.7746 - val_loss: 0.4038 - val_accuracy: 0.7325\n",
            "Epoch 26/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.2746 - accuracy: 0.7834 - val_loss: 0.4455 - val_accuracy: 0.6973\n",
            "Epoch 27/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.2655 - accuracy: 0.7925 - val_loss: 0.4362 - val_accuracy: 0.6982\n",
            "Epoch 28/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2578 - accuracy: 0.7986 - val_loss: 0.4605 - val_accuracy: 0.6865\n",
            "Epoch 29/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2543 - accuracy: 0.8032 - val_loss: 0.4243 - val_accuracy: 0.7156\n",
            "Epoch 30/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2469 - accuracy: 0.8095 - val_loss: 0.4575 - val_accuracy: 0.6753\n",
            "Epoch 31/100\n",
            "464/464 [==============================] - 19s 40ms/step - loss: 0.2403 - accuracy: 0.8176 - val_loss: 0.4521 - val_accuracy: 0.6839\n",
            "Epoch 32/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2329 - accuracy: 0.8208 - val_loss: 0.4097 - val_accuracy: 0.7135\n",
            "Epoch 33/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2242 - accuracy: 0.8314 - val_loss: 0.3814 - val_accuracy: 0.7396\n",
            "Epoch 34/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2170 - accuracy: 0.8373 - val_loss: 0.4365 - val_accuracy: 0.7084\n",
            "Epoch 35/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2089 - accuracy: 0.8489 - val_loss: 0.4034 - val_accuracy: 0.7243\n",
            "Epoch 36/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.2021 - accuracy: 0.8579 - val_loss: 0.4748 - val_accuracy: 0.6884\n",
            "Epoch 37/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1989 - accuracy: 0.8592 - val_loss: 0.3814 - val_accuracy: 0.7488\n",
            "Epoch 38/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1923 - accuracy: 0.8695 - val_loss: 0.4374 - val_accuracy: 0.7086\n",
            "Epoch 39/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1857 - accuracy: 0.8740 - val_loss: 0.4328 - val_accuracy: 0.7166\n",
            "Epoch 40/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1827 - accuracy: 0.8832 - val_loss: 0.4360 - val_accuracy: 0.7104\n",
            "Epoch 41/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1797 - accuracy: 0.8845 - val_loss: 0.4181 - val_accuracy: 0.7279\n",
            "Epoch 42/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1747 - accuracy: 0.8906 - val_loss: 0.4611 - val_accuracy: 0.7090\n",
            "Epoch 43/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1688 - accuracy: 0.8962 - val_loss: 0.4201 - val_accuracy: 0.7211\n",
            "Epoch 44/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1660 - accuracy: 0.8994 - val_loss: 0.4415 - val_accuracy: 0.7153\n",
            "Epoch 45/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1627 - accuracy: 0.9052 - val_loss: 0.4394 - val_accuracy: 0.7171\n",
            "Epoch 46/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1596 - accuracy: 0.9069 - val_loss: 0.4455 - val_accuracy: 0.7036\n",
            "Epoch 47/100\n",
            "464/464 [==============================] - 19s 41ms/step - loss: 0.1545 - accuracy: 0.9100 - val_loss: 0.4714 - val_accuracy: 0.7080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_valid)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "y_val_classes=np.argmax(y_valid, axis=1)\n",
        "\n",
        "conf = plot_confusion_matrix(y_val_classes, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "mDnpELOC5xNq",
        "outputId": "1132b833-c4ca-459c-f5da-962c683004bc"
      },
      "id": "mDnpELOC5xNq",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "tf-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "NewLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}