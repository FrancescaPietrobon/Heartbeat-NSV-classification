{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 1299.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigId</th>\n",
       "      <th>ecg_lead_1</th>\n",
       "      <th>ecg_lead_2</th>\n",
       "      <th>peaks</th>\n",
       "      <th>frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>[0.0, 0.04, 0.03, 0.0, 0.03, 0.09, 0.18, 0.14,...</td>\n",
       "      <td>[0.08, 0.07, 0.1, 0.06, 0.06, 0.03, 0.1, 0.21,...</td>\n",
       "      <td>[29, 110, 191, 272, 353, 433, 514, 595, 676, 7...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>[-0.035, -0.045, -0.025, -0.035, -0.045, -0.05...</td>\n",
       "      <td>[-0.095, -0.105, -0.095, -0.095, -0.115, -0.09...</td>\n",
       "      <td>[48, 153, 243, 352, 440, 547, 636, 742, 831, 9...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>[-0.56, -0.56, -0.55, -0.47, -0.53, -0.47, -0....</td>\n",
       "      <td>[0.43, 0.56, 0.6, 0.41, 0.54, 0.48, 0.56, 0.46...</td>\n",
       "      <td>[91, 209, 326, 394, 537, 653, 745, 872, 984, 1...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>[-0.46, -0.49, -0.52, -0.58, -0.62, -0.69, -0....</td>\n",
       "      <td>[0.56, 0.61, 0.66, 0.66, 0.63, 0.66, 0.59, 0.5...</td>\n",
       "      <td>[98, 223, 349, 474, 599, 726, 853, 980, 1116, ...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S005</td>\n",
       "      <td>[-0.27, -0.17, -0.13, -0.23, -0.18, -0.23, -0....</td>\n",
       "      <td>[-0.02, -0.04, -0.01, -0.01, -0.02, -0.06, 0.0...</td>\n",
       "      <td>[27, 127, 225, 324, 423, 523, 623, 722, 822, 9...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sigId                                         ecg_lead_1  \\\n",
       "0  S001  [0.0, 0.04, 0.03, 0.0, 0.03, 0.09, 0.18, 0.14,...   \n",
       "1  S002  [-0.035, -0.045, -0.025, -0.035, -0.045, -0.05...   \n",
       "2  S003  [-0.56, -0.56, -0.55, -0.47, -0.53, -0.47, -0....   \n",
       "3  S004  [-0.46, -0.49, -0.52, -0.58, -0.62, -0.69, -0....   \n",
       "4  S005  [-0.27, -0.17, -0.13, -0.23, -0.18, -0.23, -0....   \n",
       "\n",
       "                                          ecg_lead_2  \\\n",
       "0  [0.08, 0.07, 0.1, 0.06, 0.06, 0.03, 0.1, 0.21,...   \n",
       "1  [-0.095, -0.105, -0.095, -0.095, -0.115, -0.09...   \n",
       "2  [0.43, 0.56, 0.6, 0.41, 0.54, 0.48, 0.56, 0.46...   \n",
       "3  [0.56, 0.61, 0.66, 0.66, 0.63, 0.66, 0.59, 0.5...   \n",
       "4  [-0.02, -0.04, -0.01, -0.01, -0.02, -0.06, 0.0...   \n",
       "\n",
       "                                               peaks frequencies  \n",
       "0  [29, 110, 191, 272, 353, 433, 514, 595, 676, 7...         128  \n",
       "1  [48, 153, 243, 352, 440, 547, 636, 742, 831, 9...         128  \n",
       "2  [91, 209, 326, 394, 537, 653, 745, 872, 984, 1...         128  \n",
       "3  [98, 223, 349, 474, 599, 726, 853, 980, 1116, ...         128  \n",
       "4  [27, 127, 225, 324, 423, 523, 623, 722, 822, 9...         128  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import zipfile \n",
    "from scipy.io import loadmat \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pywt \n",
    "\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "dataset_current_folder = \"../training_set.zip\" # where the zip is\n",
    "dataset_folder = \"C://Users//simon//Desktop//AppliedAI-project\" # where I want the dataset - avoid the current folder as Git doesn't allow huge uploads\n",
    "\n",
    "with zipfile.ZipFile(dataset_current_folder, 'r') as zip: # extract the zip file into the desired folder \n",
    "    zip.extractall(dataset_folder)\n",
    "\n",
    "\n",
    "TEST = 0 # 0 if not testing (set not annotated)\n",
    "\n",
    "\n",
    "def load_data(sample_prefix, input_dir):    # everything is returned as a numpy array which is easier to manipulate\n",
    "    \n",
    "        peak_filepath = os.path.join(input_dir, sample_prefix + '_rpk.mat')\n",
    "        signal_filepath = os.path.join(input_dir, sample_prefix + '.mat')\n",
    "        if os.path.isfile(peak_filepath):\n",
    "            mat_file = loadmat(peak_filepath)\n",
    "            peaks = np.array(mat_file['rpeaks'],dtype=np.int64)\n",
    "        if os.path.isfile(signal_filepath):\n",
    "            mat_file = loadmat(signal_filepath)\n",
    "            signal = np.asarray(mat_file['ecg'] )\n",
    "\n",
    "        return peaks, signal\n",
    "\n",
    "ids = list()                # Id of samples \n",
    "rpeaks = list()             # detected peaks of the signal \n",
    "ecg_signals = list()        # .mat ecg signal \n",
    "frequencies = list()        # sample frequency of the ecg signal \n",
    "\n",
    "\n",
    "for f in os.listdir(dataset_folder):\n",
    "  if f.lower().endswith('.mat'):\n",
    "    id = f[:4]\n",
    "    if id not in ids:\n",
    "      ids.append(id)\n",
    "      sample_prefix = f[:8]\n",
    "      peak, signal = load_data(sample_prefix, dataset_folder)\n",
    "      rpeaks.append(peak)\n",
    "      ecg_signals.append(signal)\n",
    "      frequencies.append(int(sample_prefix[5:]))\n",
    "cols = [\"sigId\",\"ecg_lead_1\",\"ecg_lead_2\",\"peaks\",\"frequencies\"]\n",
    "\n",
    "# ecg signals is 105 rows [,,,,,]\n",
    "\n",
    "first_lead_signals = []\n",
    "second_lead_signals = []\n",
    "\n",
    "for signal in ecg_signals:\n",
    "    first_lead_signals.append(signal[:,0].tolist())    # converting the array to list as list of array is deprecated \n",
    "    second_lead_signals.append(signal[:,1].tolist())\n",
    "\n",
    "df = pd.DataFrame(data =[ids,first_lead_signals,second_lead_signals,rpeaks,frequencies]).T\n",
    "df.columns = cols\n",
    "\n",
    "# transform peaks \n",
    "for id in tqdm(df.index.tolist()):\n",
    "    peaks = df.iloc[id]['peaks']\n",
    "    p_list = list()\n",
    "    for p in peaks:\n",
    "        p_list.append(p[0])\n",
    "    df.iloc[id]['peaks'] = np.asarray(p_list).astype(np.int64)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:04<00:00,  8.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "ids_128 = df[df['frequencies'] == 128].index.tolist()\n",
    "ids_to_resample = df[df['frequencies'] != 128].index.tolist() \n",
    "resampled_len = len(df.iloc[ids_128[0]]['ecg_lead_1'])\n",
    "\n",
    "for id in tqdm(ids_to_resample):\n",
    "    row = df.iloc[id]\n",
    "    sampled_len = len(row['ecg_lead_1'])\n",
    "\n",
    "    # first lead\n",
    "    signal = np.asarray(row['ecg_lead_1']).astype(np.float32)\n",
    "    resampled_1 = resample(signal,resampled_len)\n",
    "    \n",
    "    # second lead\n",
    "    \n",
    "    signal = np.asarray(row['ecg_lead_2']).astype(np.float32)\n",
    "    resampled_2 = resample(signal,resampled_len)\n",
    "    \n",
    "    df.iloc[id]['ecg_lead_1'] = resampled_1.tolist()\n",
    "    df.iloc[id]['ecg_lead_2'] = resampled_2.tolist()\n",
    "    \n",
    "    for i,p in enumerate(list(df.iloc[id]['peaks'])):\n",
    "        df.iloc[id]['peaks'][i] = int(resampled_len * p/sampled_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 53.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing used in practical lessons\n",
    "from scipy.signal import resample, butter, lfilter, iirnotch\n",
    "\n",
    "def notch_filter(cutoff, fs, q=30):\n",
    "    nyq = 0.5*fs\n",
    "    freq = cutoff/nyq\n",
    "    b, a = iirnotch(freq, q)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def bandpass_filter(data, filter_order=6, lowcut = .8, highcut = 45, signal_freq=128):\n",
    "    \"\"\"\n",
    "    Method responsible for creating and applying Butterworth filter.\n",
    "    :param deque data: raw data\n",
    "    :param float lowcut: filter lowcut frequency value\n",
    "    :param float highcut: filter highcut frequency value\n",
    "    :param int signal_freq: signal frequency in samples per second (Hz)\n",
    "    :param int filter_order: filter order\n",
    "    :return array: filtered data\n",
    "    \"\"\"\n",
    "    powerline = 60\n",
    "    nyquist_freq = 0.5 * signal_freq\n",
    "    low = lowcut / nyquist_freq\n",
    "    high = highcut / nyquist_freq\n",
    "    b, a = butter(filter_order, [low, high], btype=\"band\")\n",
    "    y = lfilter(b, a, data,zi = None)\n",
    "    #b,a = notch_filter(powerline,signal_freq)\n",
    "    #y = lfilter(b,a,y)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = df.copy(deep= True)\n",
    "\n",
    "ids = filtered_df.index.tolist()\n",
    "\n",
    "for id_ in tqdm(ids):\n",
    "    \n",
    "    row = filtered_df.iloc[id_]\n",
    "    first = row['ecg_lead_1']\n",
    "    second = row['ecg_lead_2']\n",
    "    filtered_first = np.asarray(bandpass_filter(first))\n",
    "    filtered_second =  np.asarray(bandpass_filter(second))\n",
    "\n",
    "    filtered_df.iloc[id_]['ecg_lead_1'] = pd.Series(filtered_first)\n",
    "    filtered_df.iloc[id_]['ecg_lead_2'] = pd.Series(filtered_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:14<00:00,  7.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import heartpy as hp\n",
    "\n",
    "fs = 128\n",
    "time_window = 10\n",
    "samples = fs * time_window ##\n",
    "\n",
    "for id in tqdm(filtered_df.index.tolist()):\n",
    "    row = filtered_df.loc[id]\n",
    "    first_lead = row['ecg_lead_1']\n",
    "    second_lead = row['ecg_lead_2']\n",
    "    l_ = len(first_lead)\n",
    "    scaled_1 = list()\n",
    "    scaled_2 = list()\n",
    "#    \n",
    "    for i in range(int(l_/samples)):\n",
    "        to_scale_1 = first_lead[i*samples:i*samples+samples] \n",
    "        to_scale_2 = second_lead[i*samples:i*samples+samples]\n",
    "#        \n",
    "        scaled_1 += list(hp.scale_data(to_scale_1,lower=0,upper=1))\n",
    "        scaled_2 += list(hp.scale_data(to_scale_2,lower=0,upper=1))\n",
    " ##   \n",
    "    filtered_df.loc[id]['ecg_lead_1'] = scaled_1\n",
    "    filtered_df.loc[id]['ecg_lead_2'] = scaled_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 591.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR_distances</th>\n",
       "      <th>Avg_RR_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S114</th>\n",
       "      <td>[117, 116, 115, 112, 113, 114, 114, 112, 116, ...</td>\n",
       "      <td>114.914745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S115</th>\n",
       "      <td>[113, 118, 80, 151, 118, 123, 76, 152, 119, 11...</td>\n",
       "      <td>89.041006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S116</th>\n",
       "      <td>[143, 145, 141, 146, 143, 142, 144, 142, 143, ...</td>\n",
       "      <td>122.611082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S117</th>\n",
       "      <td>[142, 143, 147, 148, 145, 139, 148, 139, 140, ...</td>\n",
       "      <td>106.363048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S118</th>\n",
       "      <td>[85, 84, 86, 81, 82, 85, 81, 83, 83, 82, 84, 8...</td>\n",
       "      <td>93.967755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           RR_distances  Avg_RR_distance\n",
       "S114  [117, 116, 115, 112, 113, 114, 114, 112, 116, ...       114.914745\n",
       "S115  [113, 118, 80, 151, 118, 123, 76, 152, 119, 11...        89.041006\n",
       "S116  [143, 145, 141, 146, 143, 142, 144, 142, 143, ...       122.611082\n",
       "S117  [142, 143, 147, 148, 145, 139, 148, 139, 140, ...       106.363048\n",
       "S118  [85, 84, 86, 81, 82, 85, 81, 83, 83, 82, 84, 8...        93.967755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_ids = filtered_df.index.tolist()\n",
    "RR_dict = {}\n",
    "\n",
    "for id_ in tqdm(signal_ids):  \n",
    "  sig_Id = df.iloc[id_]['sigId']\n",
    "  RR_dict[sig_Id] = {}\n",
    "  RR_dict[sig_Id]['RR_distances'] = list()\n",
    "  \n",
    "  peaks = df.iloc[id_]['peaks']\n",
    "\n",
    "  for i,p in enumerate(peaks[1:-1]):\n",
    "    RR_distance = p-peaks[i]\n",
    "    if(RR_distance < 250): # discard outliers\n",
    "      RR_dict[sig_Id]['RR_distances'].append(RR_distance)\n",
    "\n",
    "  RR_dict[sig_Id]['RR_distances'] = np.asarray(RR_dict[sig_Id]['RR_distances'])\n",
    "\n",
    "\n",
    "  RR_dict[sig_Id]['Avg_RR_distance'] = RR_dict[sig_Id]['RR_distances'].mean()\n",
    "\n",
    "\n",
    "\n",
    "RR_df = pd.DataFrame.from_dict(RR_dict,orient=\"index\")\n",
    "RR_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heartpy as hp \n",
    "\n",
    "classes = np.array([\"N\",\"S\",\"V\"])\n",
    "\n",
    "patch_length = 350 \n",
    "\n",
    "def convert_to_one_hot(label):\n",
    "    return np.array(classes == label,dtype=np.float32)\n",
    "\n",
    "def create_patch_dataset(df):\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    ids = df['sigId']\n",
    "   \n",
    "    for id in ids:\n",
    "        row = df[df['sigId'] == id]\n",
    "        sigId = row['sigId'].values[0]\n",
    "        peaks = row['peaks'].values[0]\n",
    "        first_lead_signal = row['ecg_lead_1'].values[0]\n",
    "        avg_RR = RR_df.loc[sigId]['Avg_RR_distance'] # T for the signal \n",
    "\n",
    "        for i,peak in enumerate(peaks):\n",
    "            \n",
    "            stringIdx = str(i)    \n",
    "            dataset_dict[stringIdx] = {}\n",
    "            dataset_dict[stringIdx]['sigId'] = sigId\n",
    "            dataset_dict[stringIdx][\"first_lead\"] = list()\n",
    "            size = list(range(int(peak-(avg_RR)),int(peak+avg_RR)))\n",
    "\n",
    "            for s in size:\n",
    "                if(s < 0 or s >= len(first_lead_signal)):\n",
    "                    dataset_dict[stringIdx][\"first_lead\"].append(0.5)\n",
    "                else:\n",
    "                    dataset_dict[stringIdx][\"first_lead\"].append(first_lead_signal[s])\n",
    "            \n",
    "            first_lead = dataset_dict[stringIdx][\"first_lead\"][:]\n",
    "            \n",
    "            first_lead = resample(first_lead,patch_length)\n",
    "            \n",
    "            dataset_dict[stringIdx][\"first_lead\"] = np.asarray(first_lead[:])\n",
    "            \n",
    "\n",
    "    dataset_df = pd.DataFrame.from_dict(dataset_dict,orient=\"index\")\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_input(df):\n",
    "    x = list()\n",
    "    \n",
    "    for id in tqdm(df.index.tolist()):\n",
    "        row = df.loc[id]\n",
    "        x.append(np.transpose(np.asarray([row['first_lead']]).astype(np.float32)))\n",
    "\n",
    "    x = np.asarray(x).astype(np.float32)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 40 # 350/40 compression factor\n",
    "\n",
    "input_shape = patch_length\n",
    "\n",
    "input = keras.Input(shape=input_shape,name='Input_layer')\n",
    "\n",
    "# encoding \n",
    "\n",
    "dense1 = layers.Dense(512,activation='relu',name='Hidden_1')(input)\n",
    "dropout1 = layers.Dropout(0.1,name='Dropout_1')(dense1)\n",
    "dense2 = layers.Dense(256,activation='relu',name='Hidden_2')(dropout1)\n",
    "\n",
    "encoded = layers.Dense(encoding_dim, activation='sigmoid',name='Encoded_input')(dense2)\n",
    "\n",
    "dense3 = layers.Dense(256,activation='relu',name='Hidden_3')(encoded)\n",
    "dropout2 = layers.Dropout(0.1,name='Dropout_2')(dense3)\n",
    "dense4 = layers.Dense(512,activation='relu',name='Hidden_4')(dropout2)\n",
    "\n",
    "# decoding\n",
    "\n",
    "decoded = layers.Dense(input_shape, activation='sigmoid',name='Decoded_input')(dense4)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input, decoded)\n",
    "\n",
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x269481284c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model \n",
    "\n",
    "autoencoder.load_weights('./models/autoencoder/autoencoder_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(encoder,x_input):\n",
    "    transformed = encoder.predict(x_input)\n",
    "    return np.array(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "path = os.path.join(os.getcwd(),\"models\",\"svc\")\n",
    "\n",
    "pkl_filename = \"svc.pkl\"\n",
    "\n",
    "with open(os.path.join(path,pkl_filename), 'rb') as file:\n",
    "    sv = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import savemat\n",
    "classes = np.array([\"N\",\"S\",\"V\"])\n",
    "\n",
    "for id in tqdm(filtered_df.index.tolist()[:4]):\n",
    "    row = filtered_df.loc[id].to_frame().T\n",
    "    sigId = row['sigId'].values[0]\n",
    "    patches_df = create_patch_dataset(row)\n",
    "    predicted_labels = list()\n",
    "    x_ = patches_df['first_lead']\n",
    "    x_input = np.array(x_.values.tolist())\n",
    "    x_encoded = encoder.predict(x_input)\n",
    "    predictions = sv.predict(x_encoded)\n",
    "\n",
    "    r = df[df['sigId'] == id]\n",
    "    freq = row['frequencies'].values[0]\n",
    "    \n",
    "\n",
    "    file_path = os.path.join(os.getcwd(),'output')\n",
    "    file_name = sigId + '_' + str(freq) + '_ann.mat'\n",
    "\n",
    "    for p in predictions:\n",
    "        predicted_labels.append(classes[p])\n",
    "    \n",
    "    mat_dic = {}\n",
    "    mat_dic['labels'] = predicted_labels[:]\n",
    "\n",
    "    savemat(os.path.join(file_path,file_name), mat_dic)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2851, 350)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57032db1ad97fb6277226b3fe4aea1573f683021aa87982e2676dc61901b587a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
